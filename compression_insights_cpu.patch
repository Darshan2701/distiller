diff --git a/apputils/checkpoint.py b/apputils/checkpoint.py
index 10c1ed9..4c5b35b 100755
--- a/apputils/checkpoint.py
+++ b/apputils/checkpoint.py
@@ -85,7 +85,7 @@ def load_checkpoint(model, chkpt_file, optimizer=None):
 
     if os.path.isfile(chkpt_file):
         msglogger.info("=> loading checkpoint %s", chkpt_file)
-        checkpoint = torch.load(chkpt_file)
+        checkpoint = torch.load(chkpt_file, map_location = lambda storage, loc: storage)
         msglogger.info("Checkpoint keys:\n{}".format("\n\t".join(k for k in checkpoint.keys())))
         start_epoch = checkpoint['epoch'] + 1
         best_top1 = checkpoint.get('best_top1', None)
diff --git a/distiller/model_summaries.py b/distiller/model_summaries.py
index e04ce06..0880866 100755
--- a/distiller/model_summaries.py
+++ b/distiller/model_summaries.py
@@ -213,7 +213,8 @@ def model_performance_summary(model, dummy_input, batch_size=1):
     model = distiller.make_non_parallel_copy(model)
     model.apply(install_perf_collector)
     # Now run the forward path and collect the data
-    model(dummy_input.cuda())
+    dummy_input = dummy_input.to(distiller.model_device(model))
+    model(dummy_input)
     # Unregister from the forward hooks
     for handle in hook_handles:
         handle.remove()
diff --git a/distiller/utils.py b/distiller/utils.py
index 17d1097..69b9b15 100755
--- a/distiller/utils.py
+++ b/distiller/utils.py
@@ -25,6 +25,14 @@ import torch.nn as nn
 from copy import deepcopy
 
 
+def model_device(model):
+    """Determine the device the model is allocated on."""
+    # Source: https://discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda/180
+    if next(model.parameters()).is_cuda:
+        return 'cuda'
+    return 'cpu'
+
+
 def to_np(var):
     return var.data.cpu().numpy()
 
diff --git a/models/__init__.py b/models/__init__.py
index 7bde406..24e955f 100755
--- a/models/__init__.py
+++ b/models/__init__.py
@@ -81,5 +81,6 @@ def create_model(pretrained, dataset, arch, parallel=True, device_ids=None):
     elif parallel:
         model = torch.nn.DataParallel(model, device_ids=device_ids)
 
-    model.cuda()
+    if torch.cuda.is_available():
+        model.cuda()
     return model
